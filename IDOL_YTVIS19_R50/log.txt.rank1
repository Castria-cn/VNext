[11/28 16:35:39] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 16:35:40] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 16:35:40] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 16:35:40] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 16:35:40] detectron2.utils.env INFO: Using a generated random seed 43872338
[11/28 16:35:43] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 16:36:00] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 16:36:01] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 16:36:01] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 16:36:03] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 16:36:15] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 153, in train
    self.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 279, in run_step
    loss_dict = self.model(data)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/idol.py", line 229, in forward
    output, loss_dict = self.detr(images, det_targets,ref_targets, self.criterion, train=True)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/segmentation_condInst.py", line 214, in forward
    contrast_items = select_pos_neg(inter_references_ref[-1], matched_ids, ref_targets,det_targets, self.reid_embed_head, hs[-1], hs_ref[-1], ref_cls)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/pos_neg_select.py", line 34, in select_pos_neg
    logger.log_id(f'ref_box_bz shape={ref_box_bz.shape}', 1)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/util/ot_logger.py", line 17, in log_id
    with open(self.log_file, 'a') as f:
FileNotFoundError: [Errno 2] No such file or directory: '~/VNext/ot_log.txt'
[11/28 16:36:15] detectron2.engine.hooks INFO: Total training time: 0:00:11 (0:00:00 on hooks)
[11/28 16:40:48] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 16:40:48] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 16:40:48] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 16:40:48] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 16:40:49] detectron2.utils.env INFO: Using a generated random seed 52577637
[11/28 16:40:52] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 16:41:06] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 16:41:06] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 16:41:06] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 16:41:06] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 17:32:52] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 17:32:53] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 17:32:53] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 17:32:53] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 17:32:53] detectron2.utils.env INFO: Using a generated random seed 56661007
[11/28 17:32:56] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 17:33:10] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 17:33:10] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 17:33:10] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 17:33:11] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 17:35:51] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 17:35:51] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 17:35:51] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 17:35:51] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 17:35:51] detectron2.utils.env INFO: Using a generated random seed 55581319
[11/28 17:35:55] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 17:36:09] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 17:36:09] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 17:36:09] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 17:36:10] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 17:36:22] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 153, in train
    self.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 279, in run_step
    loss_dict = self.model(data)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/idol.py", line 229, in forward
    output, loss_dict = self.detr(images, det_targets,ref_targets, self.criterion, train=True)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/segmentation_condInst.py", line 214, in forward
    contrast_items = select_pos_neg(inter_references_ref[-1], matched_ids, ref_targets,det_targets, self.reid_embed_head, hs[-1], hs_ref[-1], ref_cls)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/pos_neg_select.py", line 42, in select_pos_neg
    logger.log_id(f'positive shape: {pos_idx.shape}, negative shape: {neg_idx.shape}', 5)
AttributeError: 'list' object has no attribute 'shape'
[11/28 17:36:22] detectron2.engine.hooks INFO: Total training time: 0:00:12 (0:00:00 on hooks)
[11/28 17:37:51] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 17:37:52] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 17:37:52] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 17:37:52] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 17:37:52] detectron2.utils.env INFO: Using a generated random seed 55937785
[11/28 17:37:55] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 17:38:09] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 17:38:09] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 17:38:09] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 17:38:10] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 17:41:52] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 17:41:52] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 17:41:52] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 17:41:52] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 17:41:52] detectron2.utils.env INFO: Using a generated random seed 56273570
[11/28 17:41:56] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 17:42:10] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 17:42:10] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 17:42:10] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 17:42:10] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 17:44:20] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 17:44:21] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 17:44:21] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 17:44:21] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 17:44:21] detectron2.utils.env INFO: Using a generated random seed 25247238
[11/28 17:44:24] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 17:44:39] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 17:44:39] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 17:44:40] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 17:44:40] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 17:44:51] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 153, in train
    self.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 279, in run_step
    loss_dict = self.model(data)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/idol.py", line 229, in forward
    output, loss_dict = self.detr(images, det_targets,ref_targets, self.criterion, train=True)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/segmentation_condInst.py", line 214, in forward
    contrast_items = select_pos_neg(inter_references_ref[-1], matched_ids, ref_targets,det_targets, self.reid_embed_head, hs[-1], hs_ref[-1], ref_cls)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/pos_neg_select.py", line 42, in select_pos_neg
    pos_idx = torch.stack(pos_idx)
TypeError: expected Tensor as element 0 in argument 0, but got NoneType
[11/28 17:44:51] detectron2.engine.hooks INFO: Total training time: 0:00:11 (0:00:00 on hooks)
[11/28 17:50:51] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 17:50:52] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 17:50:52] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 17:50:52] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 17:50:52] detectron2.utils.env INFO: Using a generated random seed 55706393
[11/28 17:50:55] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 17:51:09] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 17:51:09] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 17:51:09] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 17:51:10] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 17:51:23] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 153, in train
    self.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 279, in run_step
    loss_dict = self.model(data)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/idol.py", line 229, in forward
    output, loss_dict = self.detr(images, det_targets,ref_targets, self.criterion, train=True)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/segmentation_condInst.py", line 214, in forward
    contrast_items = select_pos_neg(inter_references_ref[-1], matched_ids, ref_targets,det_targets, self.reid_embed_head, hs[-1], hs_ref[-1], ref_cls)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/pos_neg_select.py", line 42, in select_pos_neg
    pos_idx = torch.stack(pos_idx)
TypeError: expected Tensor as element 0 in argument 0, but got NoneType
[11/28 17:51:23] detectron2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[11/28 18:08:52] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 18:08:52] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 18:08:52] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 18:08:52] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 18:08:52] detectron2.utils.env INFO: Using a generated random seed 56372848
[11/28 18:08:56] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 18:09:10] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 18:09:10] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 18:09:10] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 18:09:10] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 18:09:22] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 153, in train
    self.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 279, in run_step
    loss_dict = self.model(data)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/idol.py", line 229, in forward
    output, loss_dict = self.detr(images, det_targets,ref_targets, self.criterion, train=True)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/segmentation_condInst.py", line 190, in forward
    self.logger.log_id(f'hs_ref shape = {hs_ref.shape}, hs_ref[-1, pred_i] shape = {hs_ref[-1, pred_i].shape}', 1)
IndexError: The shape of the mask [300] at index 0 does not match the shape of the indexed tensor [4, 300, 256] at index 0
[11/28 18:09:22] detectron2.engine.hooks INFO: Total training time: 0:00:11 (0:00:00 on hooks)
[11/28 18:11:52] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 18:11:52] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 18:11:52] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 18:11:52] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 18:11:52] detectron2.utils.env INFO: Using a generated random seed 56330938
[11/28 18:11:56] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 18:12:09] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 18:12:10] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 18:12:10] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 18:12:10] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 18:17:52] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 18:17:52] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 18:17:52] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 18:17:52] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 18:17:52] detectron2.utils.env INFO: Using a generated random seed 56529240
[11/28 18:17:56] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 18:18:10] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 18:18:11] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 18:18:11] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 18:18:11] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 18:18:22] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 153, in train
    self.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 279, in run_step
    loss_dict = self.model(data)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/idol.py", line 229, in forward
    output, loss_dict = self.detr(images, det_targets,ref_targets, self.criterion, train=True)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/segmentation_condInst.py", line 218, in forward
    contrast_items = select_pos_neg(inter_references_ref[-1], matched_ids, ref_targets,det_targets, self.reid_embed_head, hs[-1], hs_ref[-1], ref_cls)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/pos_neg_select.py", line 42, in select_pos_neg
    pos_idx = torch.stack(pos_idx)
TypeError: expected Tensor as element 0 in argument 0, but got NoneType
[11/28 18:18:22] detectron2.engine.hooks INFO: Total training time: 0:00:11 (0:00:00 on hooks)
[11/28 18:35:22] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 18:35:23] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 18:35:23] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 18:35:23] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 18:35:23] detectron2.utils.env INFO: Using a generated random seed 26641844
[11/28 18:35:26] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 18:35:40] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 18:35:40] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 18:35:40] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 18:35:40] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 18:35:52] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 153, in train
    self.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 279, in run_step
    loss_dict = self.model(data)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/idol.py", line 229, in forward
    output, loss_dict = self.detr(images, det_targets,ref_targets, self.criterion, train=True)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/segmentation_condInst.py", line 199, in forward
    self.queue.enqueue(item)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1695, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'CondInst_segm' object has no attribute 'queue'
[11/28 18:35:52] detectron2.engine.hooks INFO: Total training time: 0:00:11 (0:00:00 on hooks)
[11/28 18:37:20] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 18:37:21] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 18:37:21] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 18:37:21] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 18:37:21] detectron2.utils.env INFO: Using a generated random seed 24903128
[11/28 18:37:24] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 18:37:38] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 18:37:38] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 18:37:38] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 18:37:38] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 18:37:50] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 153, in train
    self.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 279, in run_step
    loss_dict = self.model(data)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/idol.py", line 229, in forward
    output, loss_dict = self.detr(images, det_targets,ref_targets, self.criterion, train=True)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/segmentation_condInst.py", line 228, in forward
    contrast_items = select_pos_neg(inter_references_ref[-1], matched_ids, ref_targets,det_targets, self.reid_embed_head, hs[-1], hs_ref[-1], ref_cls, self.queue)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1695, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'CondInst_segm' object has no attribute 'queue'
[11/28 18:37:50] detectron2.engine.hooks INFO: Total training time: 0:00:11 (0:00:00 on hooks)
[11/28 18:40:23] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 18:40:23] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 18:40:23] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 18:40:23] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 18:40:24] detectron2.utils.env INFO: Using a generated random seed 27657547
[11/28 18:40:27] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 18:40:41] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 18:40:41] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 18:40:42] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 18:40:42] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 18:53:52] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 18:53:52] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 18:53:52] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 18:53:52] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 18:53:52] detectron2.utils.env INFO: Using a generated random seed 56344697
[11/28 18:53:56] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 18:54:10] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 18:54:10] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 18:54:11] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 18:54:11] detectron2.engine.train_loop INFO: Starting Training from iteration 0
[11/28 18:54:22] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 153, in train
    self.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/vhome/wangyaoning/VNext/detectron2/engine/train_loop.py", line 279, in run_step
    loss_dict = self.model(data)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/idol.py", line 229, in forward
    output, loss_dict = self.detr(images, det_targets,ref_targets, self.criterion, train=True)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/segmentation_condInst.py", line 228, in forward
    contrast_items = select_pos_neg(inter_references_ref[-1], matched_ids, ref_targets,det_targets, self.reid_embed_head, hs[-1], hs_ref[-1], ref_cls, self.object_queue)
  File "/vhome/wangyaoning/VNext/projects/IDOL/idol/models/pos_neg_select.py", line 58, in select_pos_neg
    sword_pos = ref_embed[bz_i][contrastive_pos[0][inst_i]]
NameError: name 'ref_embed' is not defined
[11/28 18:54:22] detectron2.engine.hooks INFO: Total training time: 0:00:11 (0:00:00 on hooks)
[11/28 18:56:21] detectron2 INFO: Rank of current process: 1. World size: 4
[11/28 18:56:22] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------
sys.platform            linux
Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
numpy                   1.26.1
detectron2              0.6 @/vhome/wangyaoning/VNext/detectron2
Compiler                GCC 11.4
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 2.1.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          535.54.03
CUDA_HOME               /share/apps/cuda/12.2
Pillow                  10.1.0
torchvision             0.16.0+cu121 @/vhome/wangyaoning/.local/lib/python3.10/site-packages/torchvision
torchvision arch flags  5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.8.1
----------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 18:56:22] detectron2 INFO: Command line arguments: Namespace(config_file='projects/IDOL/configs/ytvis19_r50.yaml', resume=False, eval_only=False, num_gpus=4, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:59279', opts=[])
[11/28 18:56:22] detectron2 INFO: Contents of args.config_file=projects/IDOL/configs/ytvis19_r50.yaml:
MODEL:
  META_ARCHITECTURE: "IDOL"
  WEIGHTS: "model_final.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: True
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  IDOL:
    NUM_CLASSES: 40
    MULTI_CLS_ON: True
DATASETS:
  TRAIN: ("ytvis_2019_train",)
  TEST: ("ytvis_2019_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (8000,)
  MAX_ITER: 12000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  CHECKPOINT_PERIOD: 1000
INPUT:
  SAMPLING_FRAME_NUM: 2
  SAMPLING_FRAME_RANGE:  10
  # MIN_SIZE_TRAIN_SAMPLING : ["range", "choice", "range_by_clip", "choice_by_clip"]
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"
  # RANDOM_FLIP : ["none", "horizontal", "flip_by_clip"]. "horizontal" is set by default.
  RANDOM_FLIP: "flip_by_clip"
  # AUGMENTATIONS: []
  # MIN_SIZE_TRAIN: (360, 480)
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MAX_SIZE_TRAIN: 768
  MIN_SIZE_TEST: 480
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2
OUTPUT_DIR: ./IDOL_YTVIS19_R50

[11/28 18:56:22] detectron2.utils.env INFO: Using a generated random seed 26191567
[11/28 18:56:25] detectron2.engine.defaults INFO: Model:
IDOL(
  (detr): CondInst_segm(
    (detr): DeformableDETR(
      (transformer): DeformableTransformer(
        (encoder): DeformableTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (decoder): DeformableTransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x DeformableTransformerDecoderLayer(
              (cross_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (dropout2): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout4): Dropout(p=0.1, inplace=False)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (bbox_embed): ModuleList(
            (0-5): 6 x MLP(
              (layers): ModuleList(
                (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
                (2): Linear(in_features=256, out_features=4, bias=True)
              )
            )
          )
        )
        (reference_points): Linear(in_features=256, out_features=2, bias=True)
      )
      (class_embed): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=40, bias=True)
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (query_embed): Embedding(300, 512)
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (3): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (backbone): Joiner(
        (0): MaskedBackbone(
          (backbone): ResNet(
            (stem): BasicStem(
              (conv1): Conv2d(
                3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
            )
            (res2): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv1): Conv2d(
                  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv2): Conv2d(
                  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
                )
                (conv3): Conv2d(
                  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
              )
            )
            (res3): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv1): Conv2d(
                  256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv2): Conv2d(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
                )
                (conv3): Conv2d(
                  128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
              )
            )
            (res4): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
                (conv1): Conv2d(
                  512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (3): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (4): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
              (5): BottleneckBlock(
                (conv1): Conv2d(
                  1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv2): Conv2d(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
                )
                (conv3): Conv2d(
                  256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
                )
              )
            )
            (res5): Sequential(
              (0): BottleneckBlock(
                (shortcut): Conv2d(
                  1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
                (conv1): Conv2d(
                  1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (1): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
              (2): BottleneckBlock(
                (conv1): Conv2d(
                  2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv2): Conv2d(
                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
                )
                (conv3): Conv2d(
                  512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
                )
              )
            )
          )
        )
        (1): PositionEmbeddingSine()
      )
    )
    (controller): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=169, bias=True)
      )
    )
    (mask_head): MaskHeadSmallConv(
      (lay1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay2): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (lay4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dcn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (reid_embed_head): MLP(
      (layers): ModuleList(
        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[11/28 18:56:40] detectron2.data.common INFO: Serializing 2238 elements to byte tensors and concatenating them all ...
[11/28 18:56:40] detectron2.data.common INFO: Serialized dataset takes 150.26 MiB
[11/28 18:56:40] fvcore.common.checkpoint INFO: [Checkpointer] Loading from model_final.pth ...
[11/28 18:56:41] detectron2.engine.train_loop INFO: Starting Training from iteration 0
